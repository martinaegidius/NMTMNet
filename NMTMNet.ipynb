{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7dr_b1I-scN_"
      },
      "source": [
        "Setup"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Kqs1Fu2bsO6Z"
      },
      "outputs": [],
      "source": [
        "!pip install -q proteinworkshop\n",
        "!pip install -q torch==2.0.0+cu118 torchvision==0.15.1+cu118 torchaudio==2.0.1 --index-url https://download.pytorch.org/whl/cu118 --no-cache-dir\n",
        "!python3 /usr/local/lib/python3.10/dist-packages/proteinworkshop/scripts/install_pyg.py\n",
        "!export DATA_PATH=\"proteinworkshop/data/\"\n",
        "!pip install -q graphein\n",
        "!pip uninstall gdown -y && pip install gdown\n",
        "!gdown -V\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!git clone https://github.com/martinaegidius/NMTMNet"
      ],
      "metadata": {
        "id": "v7UPFzTY-ltL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_C8KAzQ7si_y"
      },
      "outputs": [],
      "source": [
        "from proteinworkshop.features.factory import ProteinFeaturiser\n",
        "from proteinworkshop.datasets.utils import create_example_batch\n",
        "from graphein.protein.utils import download_alphafold_structure\n",
        "from torch_geometric.loader import DataLoader\n",
        "import pickle\n",
        "from NMTMNet.transmembraneDataset import transmembraneDataset\n",
        "import NMTMNet.transmembraneUtils as tmu\n",
        "import torch\n",
        "from NMTMNet.transmembraneModels import GraphEncDec\n",
        "from NMTMNet.transmembraneModels import init_linear_weights\n",
        "import os\n",
        "from torch_geometric.utils import unbatch\n",
        "import wandb\n",
        "import numpy as np\n",
        "import torchmetrics\n",
        "import copy\n",
        "from torch.utils.data import WeightedRandomSampler\n",
        "import json\n",
        "import math\n",
        "import torch.optim.lr_scheduler as lr_scheduler"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fFZN44oXskmq"
      },
      "outputs": [],
      "source": [
        "###download dataset\n",
        "import gdown\n",
        "dir_processed,dir_raw = gdown.download_folder(url=\"https://drive.google.com/drive/folders/1dUCoN0khw_G7c5_C7fN-TPxaMiKJYMgb?usp=drive_link\",quiet=True)\n",
        "#!gdown https://drive.google.com/file/d/1krOtmE5N_kEXqYzlOAkYoUe4-861oOlq/view?usp=sharing -O /content/NMTMNet/data/ #download tensors\n",
        "#!gdown https://drive.google.com/file/d/1kzVGthEXfAHbNE9O7MoWXyh6fYX_gnLd/view?usp=sharing -O /NMTMNet/data/ #download pdbs - may not be necessary for dataloader unless you reflush\n",
        "#!unzip /content/NMTMNet/data/processed.zip\n",
        "#!unzip /NMTMNet/data/raw.zip\n",
        "\n",
        "#!7z x /content/data/processed.zip\n",
        "!unzip -qq /content/data/processed.zip -d NMTMNet/data/\n",
        "!unzip -qq /content/data/raw.zip -d NMTMNet/data/\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Training loop"
      ],
      "metadata": {
        "id": "pjOEUrLmKYbY"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IrFqW7hcsvQd"
      },
      "outputs": [],
      "source": [
        "torch.cuda.empty_cache()\n",
        "### ------------------ Config flags ---------------------- ###\n",
        "with open(\"NMTMNet/param_cfg.json\") as f:\n",
        "  param_cfg = json.load(f)\n",
        "\n",
        "print_every = 3\n",
        "eval_every = 5\n",
        "\n",
        "### ------------------ End Config flags ---------------------- ###\n",
        "\n",
        "#train: cv-folds 0,3,4\n",
        "#val: cv-fold 1\n",
        "#test: cv-fold 2\n",
        "with open(\"NMTMNet/splits/new/train.pkl\",'rb') as f:\n",
        "    train = pickle.load(f)\n",
        "\n",
        "with open(\"NMTMNet/splits/new/val.pkl\",'rb') as f:\n",
        "    val = pickle.load(f)\n",
        "\n",
        "with open(\"NMTMNet/splits/new/test.pkl\",'rb') as f:\n",
        "    test = pickle.load(f)\n",
        "\n",
        "#exclude mismatching proteins\n",
        "with open(\"NMTMNet/data_quality_control.pkl\",\"rb\") as f:\n",
        "    mismatches = pickle.load(f)\n",
        "\n",
        "\n",
        "mismatches = mismatches[\"Different sequence\"]\n",
        "trainnames = [x[\"id\"] for x in train]\n",
        "testnames = [x[\"id\"] for x in test]\n",
        "valnames = [x[\"id\"] for x in val]\n",
        "\n",
        "\n",
        "\n",
        "trainlabels = {}\n",
        "for prot in train:\n",
        "    trainlabels[prot[\"id\"]]=prot[\"labels\"]\n",
        "\n",
        "vallabels = {}\n",
        "for prot in val:\n",
        "    vallabels[prot[\"id\"]]=prot[\"labels\"]\n",
        "\n",
        "testlabels = {}\n",
        "for prot in test:\n",
        "    testlabels[prot[\"id\"]]=prot[\"labels\"]\n",
        "\n",
        "\n",
        "torch.manual_seed(param_cfg[\"SEED\"])\n",
        "\n",
        "\n",
        "\n",
        "pdb_dir = \"NMTMNet/data/\" #downloaded using AF2.4\n",
        "\n",
        "\n",
        "complete_labels = {}\n",
        "complete_labels.update(trainlabels)\n",
        "complete_labels.update(vallabels)\n",
        "complete_labels.update(testlabels)\n",
        "complete_names = trainnames+testnames+valnames\n",
        "\n",
        "\n",
        "\n",
        "#download dataset - this is handled using google drive download, this is just an illustrative example\n",
        "# def download_matching_protein(protein_name,path):\n",
        "#     #protein_path = download_alphafold_structure(protein_name, out_dir = path, aligned_score=True)\n",
        "#     protein_path = download_alphafold_structure(protein_name, version=4,out_dir = path, aligned_score=True)\n",
        "#     if(protein_path==None):#case: download failed\n",
        "#         success = 0\n",
        "#     else:\n",
        "#         success = 1\n",
        "#     return [protein_path,success]\n",
        "\n",
        "# for name in complete_names: #note - this takes a long time\n",
        "#   download_matching_protein(name,pdb_dir)\n",
        "\n",
        "\n",
        "\n",
        "completeDset = transmembraneDataset(root=pdb_dir,proteinlist=complete_names,labelDict=complete_labels,mismatching_proteins=mismatches,flush_files=False)  #this also is slow\n",
        "#get pairing of protein names and indices in the dset:\n",
        "# protein_indices = {} #store index-name-pairs\n",
        "# for idx, data in enumerate(completeDset):\n",
        "#     protein_name = data.id  # Replace with the attribute that holds protein names\n",
        "#     if protein_name not in protein_indices:\n",
        "#         protein_indices[protein_name] = []\n",
        "#     protein_indices[protein_name].append(idx)\n",
        "\n",
        "# with open(\"protein_index_pairing.json\",\"w\") as f:\n",
        "#    json.dump(protein_indices,f)\n",
        "\n",
        "#test that approach works\n",
        "# train_indices = []\n",
        "# val_indices = []\n",
        "# test_indices = []\n",
        "\n",
        "# for protein in trainnames:\n",
        "#    train_indices.extend(protein_indices[protein])\n",
        "\n",
        "# for protein in valnames:\n",
        "#    val_indices.extend(protein_indices[protein])\n",
        "\n",
        "# for protein in testnames:\n",
        "#    test_indices.extend(protein_indices[protein])\n",
        "\n",
        "# trainSet = torch.utils.data.Subset(completeDset,train_indices)\n",
        "# valSet = torch.utils.data.Subset(completeDset,val_indices)\n",
        "# testSet = torch.utils.data.Subset(completeDset,test_indices)\n",
        "\n",
        "\n",
        "type2key = {'I': 0, 'O':1, 'P': 2, 'S': 3, 'M':4, 'B': 5}  #theirs\n",
        "\n",
        "# # #adding weighted sampler to counter hefty class-inbalance - the following code is only to generate the class-weights and the weights-tensor, it is rather slow so don't run it again\n",
        "# classCounter = [0 for x in range(5)]\n",
        "# label_sampler_li = []\n",
        "# for idx in trainSet.indices():\n",
        "#    print(f\"{idx}/{len(trainSet.indices())}\")\n",
        "#    label = trainSet[idx].label\n",
        "#    label = tmu.label_to_tensor(label,type2key)\n",
        "#    prot_type,_ = tmu.type_from_labels(label)\n",
        "#    classCounter[prot_type] += 1\n",
        "#    label_sampler_li.append(prot_type)\n",
        "# print(classCounter)\n",
        "# weights = 1/np.array(classCounter)\n",
        "# samples_weight = np.array([weights[t] for t in label_sampler_li])\n",
        "# print(\"samples weight: \",samples_weight)\n",
        "# samples_weight = torch.from_numpy(samples_weight)\n",
        "# samples_weight = samples_weight.double()\n",
        "# print(samples_weight)\n",
        "# torch.save(samples_weight,\"sampling_weights.pt\")\n",
        "# print(\"Saved weights to scratch!\")\n",
        "# with open(\"index_class_labels.pkl\",\"wb\") as f:\n",
        "#    pickle.dump(label_sampler_li,f)\n",
        "# print(\"Saved class labels list to scratch in case you want to change the weighting principle!\")\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "with open(\"NMTMNet/protein_index_pairing.json\",\"r\") as f:\n",
        "   protein_indices = json.load(f)\n",
        "\n",
        "train_indices = []\n",
        "val_indices = []\n",
        "test_indices = []\n",
        "\n",
        "for protein in trainnames:\n",
        "   protein = protein + \"_ABCD\"\n",
        "   train_indices.extend(protein_indices[protein])\n",
        "\n",
        "for protein in valnames:\n",
        "   protein = protein+ \"_ABCD\"\n",
        "   val_indices.extend(protein_indices[protein])\n",
        "\n",
        "for protein in testnames:\n",
        "   protein = protein+ \"_ABCD\"\n",
        "   test_indices.extend(protein_indices[protein])\n",
        "\n",
        "trainSet = torch.utils.data.Subset(completeDset,train_indices)\n",
        "valSet = torch.utils.data.Subset(completeDset,val_indices)\n",
        "testSet = torch.utils.data.Subset(completeDset,test_indices)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "type2key = {'I': 0, 'O':1, 'P': 2, 'S': 3, 'M':4, 'B': 5}  #theirs\n",
        "if(param_cfg[\"NUM_SAMPLES\"]!=\"ALL\"):\n",
        "    print(\"DETECTED REQUEST FOR SUBSET OF DATA\")\n",
        "    subsetIdx = torch.randperm(len(trainSet))\n",
        "    trainSet = torch.utils.data.Subset(trainSet,subsetIdx[1:param_cfg[\"NUM_SAMPLES\"]+1]) ##for getting a decent-ish split\n",
        "    subsetIdx = torch.randperm(len(valSet))\n",
        "    valSet = torch.utils.data.Subset(valSet,subsetIdx[1:param_cfg[\"NUM_SAMPLES\"]+1])\n",
        "    subsetIdx = torch.randperm(len(testSet))\n",
        "    testSet = torch.utils.data.Subset(testSet,subsetIdx[1:param_cfg[\"NUM_SAMPLES\"]+1])\n",
        "\n",
        "elif(param_cfg[\"WEIGHTEDSAMPLING\"]==True):\n",
        "   sampler_weights = torch.load(\"NMTMNet/sampling_weights.pt\")\n",
        "   print(\"Loaded sampling weights from scratch. Unique values and counts are:\")\n",
        "   print(torch.unique(sampler_weights,return_counts=True,sorted=True))\n",
        "   if(isinstance(param_cfg[\"WEIGHTMODULATOR\"],list)):\n",
        "      with open(\"NMTMNet/index_class_labels.pkl\",\"rb\") as f:\n",
        "         class_indices = pickle.load(f)\n",
        "\n",
        "      print(\"Detected non-uniform weight-modulation\")\n",
        "      for i, weight in enumerate(sampler_weights):\n",
        "         #if(class_indices[i]==0):\n",
        "            #print(f\"Detected class TM at index {i}\")\n",
        "         sampler_weights[i] = weight*param_cfg[\"WEIGHTMODULATOR\"][class_indices[i]]\n",
        "      print(\"Unique values and counts for sampler after modulation: \")\n",
        "      print(torch.unique(sampler_weights,return_counts=True,sorted=True))\n",
        "   sampler = WeightedRandomSampler(sampler_weights, len(sampler_weights),replacement=True)\n",
        "\n",
        "\n",
        "\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "\n",
        "\n",
        "if(param_cfg[\"WEIGHTEDSAMPLING\"]==True):\n",
        "   trainloader = DataLoader(trainSet,batch_size=param_cfg[\"BATCH_SZ\"],shuffle=False,num_workers=0,sampler=sampler)\n",
        "else:\n",
        "   trainloader = DataLoader(trainSet,batch_size=param_cfg[\"BATCH_SZ\"],shuffle=True,num_workers=0) #need to set true when finished overfitting!\n",
        "valloader = DataLoader(valSet,batch_size=param_cfg[\"BATCH_SZ\"],shuffle=False,num_workers=0)\n",
        "testloader = DataLoader(testSet,batch_size=param_cfg[\"BATCH_SZ\"],shuffle=False,num_workers=0)\n",
        "\n",
        "\n",
        "if(param_cfg[\"Featuriser\"]==\"SIMPLE\"):\n",
        "  featuriser = ProteinFeaturiser( #note: input is a protein Batch\n",
        "          representation=\"CA\",\n",
        "          scalar_node_features=[\"amino_acid_one_hot\"],\n",
        "          vector_node_features=[],\n",
        "          edge_types=[\"knn_16\"],\n",
        "          scalar_edge_features=[\"edge_distance\"],\n",
        "          vector_edge_features=[],\n",
        "        )\n",
        "\n",
        "if(param_cfg[\"Featuriser\"]==\"INTERMEDIATE\"):\n",
        "  featuriser = ProteinFeaturiser( #note: input is a protein Batch\n",
        "          representation=\"CA\",\n",
        "          scalar_node_features=[\"amino_acid_one_hot\",\"sequence_positional_encoding\",\"alpha\",\"kappa\",\"dihedrals\"],\n",
        "          vector_node_features=[],\n",
        "          edge_types=[\"knn_16\"],\n",
        "          scalar_edge_features=[\"edge_distance\"],\n",
        "          vector_edge_features=[],\n",
        "        )\n",
        "\n",
        "if(param_cfg[\"Featuriser\"]==\"COMPLEX\"):#in general, according to paper, this should work less well than INTERMEDIATE\n",
        "  featuriser = ProteinFeaturiser( #note: input is a protein Batch\n",
        "          representation=\"CA\",\n",
        "          scalar_node_features=[\"amino_acid_one_hot\",\"sequence_positional_encoding\",\"alpha\",\"kappa\",\"dihedrals\",\"sidechain_torsions\"],\n",
        "          vector_node_features=[],\n",
        "          edge_types=[\"knn_16\"],\n",
        "          scalar_edge_features=[\"edge_distance\"],\n",
        "          vector_edge_features=[],\n",
        "        )\n",
        "\n",
        "\n",
        "\n",
        "###--------------------------- Start training -------------------------\n",
        "if(param_cfg[\"TRACKING\"]==True):\n",
        "  wandb.login(key=\"INSERT YOUR API-KEY-HERE\")\n",
        "  run = wandb.init(project=param_cfg[\"experimentType\"],config=param_cfg)\n",
        "\n",
        "model = GraphEncDec(featuriser=featuriser, n_classes=6,hidden_dim_GCN=param_cfg[\"GCSize\"],decoderType=param_cfg[\"DecoderType\"],LSTM_hidden_dim=param_cfg[\"LSTMSize\"],dropout=param_cfg[\"Dropout\"],LSTMnormalization = param_cfg[\"LSTMNORM\"],lstm_layers=param_cfg[\"LSTMLAYERS\"])\n",
        "print(model)\n",
        "model.apply(init_linear_weights) #change to xavier normal init\n",
        "model = model.to(device)\n",
        "optimizer = torch.optim.Adam(model.parameters(),lr=param_cfg[\"LR\"],weight_decay=param_cfg[\"WEIGHTDECAY\"],)\n",
        "criterion = torch.nn.CrossEntropyLoss(label_smoothing = param_cfg[\"LABELSMOOTHING\"])\n",
        "if(param_cfg[\"OPTIMSCHEDULE\"]==\"LINEAR\"):\n",
        "   scheduler = lr_scheduler.LinearLR(optimizer, start_factor=1.0, end_factor = 0.1,total_iters=param_cfg[\"N_EPOCHS\"]-20)\n",
        "epoch_loss = []\n",
        "epoch_val_loss = []\n",
        "epoch_train_acc_overlap = []\n",
        "epoch_train_acc = []\n",
        "epoch_val_acc_overlap = []\n",
        "epoch_val_acc = []\n",
        "\n",
        "\n",
        "###MANAGE TRACKING OF MODEL WEIGHTS, PREDICTIONS AND LABELS\n",
        "if(param_cfg[\"TRACKING\"]==True):\n",
        "   wandb.watch(model, log_freq=100)\n",
        "\n",
        "#make a table hook to save predictions\n",
        "def log_prediction_table(epoch,proteinName,label,prediction,accuracy,overlap_match,protein_label,protein_prediction,type):\n",
        "    table = wandb.Table(columns=[\"epoch\",\"protein\",\"label\",\"prediction\",\"accuracy\",\"overlap\",\"type label\",\"prediction label\"])\n",
        "    for epoch, name, label, pred, acc, overlap_match_, protein_lab,protein_pred in zip(epoch,proteinName,label,prediction,accuracy,overlap_match,protein_label,protein_prediction):\n",
        "        table.add_data(epoch,name,label,pred,acc,overlap_match_,protein_lab,protein_pred)\n",
        "    wandb.log({f\"{type}/predictions_table\":table},commit=False)\n",
        "\n",
        "\n",
        "train_data_log = next(iter(trainloader)) #probably make a random sample subset instead at some point\n",
        "\n",
        "gradclip = param_cfg[\"CLIPGRADS\"]\n",
        "if(gradclip):\n",
        "   clipval = param_cfg[\"CLIPVAL\"]\n",
        "else:\n",
        "   clipval = None\n",
        "\n",
        "\n",
        "def gradNorm(model):\n",
        "   grads = [\n",
        "   param.grad.detach().flatten()\n",
        "   for param in model.parameters()\n",
        "   if param.grad is not None\n",
        "   ]\n",
        "   norm = torch.cat(grads).norm()\n",
        "   return norm\n",
        "\n",
        "class EarlyStopping:\n",
        "    def __init__(self, tolerance=30,path=\"\"):\n",
        "        self.tolerance = tolerance\n",
        "        self.counter = 0\n",
        "        self.early_stop = False\n",
        "        self.max_Oacc = -1.0\n",
        "        self.max_acc = -1.0\n",
        "        self.modelCheckPoint = None\n",
        "        self.path = path\n",
        "\n",
        "    def __call__(self, overlapAcc,acc,model):\n",
        "        #print(\"Holding minimum loss: \", self.min_loss)\n",
        "        #print(\"received: \",validation_loss)\n",
        "        #print(\"with type: \",type(validation_loss))\n",
        "        if(overlapAcc>=self.max_Oacc and acc>=self.max_acc):\n",
        "           self.modelCheckPoint = copy.deepcopy(model.state_dict())\n",
        "           self.max_Oacc = overlapAcc\n",
        "           self.max_acc = acc\n",
        "           self.counter = 0\n",
        "        else:\n",
        "            self.counter +=1\n",
        "            if self.counter >= self.tolerance:\n",
        "                self.early_stop = True\n",
        "                print(\"EARLY STOPPING TRIGGERED.\")\n",
        "                self.saveModel()\n",
        "\n",
        "    def saveModel(self):\n",
        "       modPath = self.path+\"checkpoint.pt\"\n",
        "       torch.save(self.modelCheckPoint, modPath)\n",
        "       print(\"Saved best model checkpoint to: \", modPath)\n",
        "       return\n",
        "\n",
        "if(param_cfg[\"EARLYSTOP\"]):\n",
        "   early_stopping = EarlyStopping(tolerance=30)\n",
        "\n",
        "\n",
        "for i in range(param_cfg[\"N_EPOCHS\"]):\n",
        "    loss, output = tmu.train_single_epoch(model,optimizer,criterion,trainloader,type2key,gradclip,clipval)\n",
        "    if(param_cfg[\"BATCH_SZ\"]==1):\n",
        "      loss /= len(trainSet)\n",
        "    else:\n",
        "      loss /= len(trainloader) #number of batches in loader, slightly positively biased if the number of batches is not equal\n",
        "    epoch_loss.append(loss)\n",
        "\n",
        "    grad_norm = gradNorm(model)\n",
        "    train_metrics = {\"train/loss\":loss,\n",
        "                     \"train/epoch\":i,\n",
        "                     \"train/Grad-L2\":grad_norm}\n",
        "    if(param_cfg[\"TRACKING\"]==True):\n",
        "       wandb.log(train_metrics)\n",
        "    if(i%print_every==0):\n",
        "      print(f\"Loss in epoch {i}: {loss}\")\n",
        "      print(f\"Gradient L2 norm in epoch {i} is {grad_norm}\")\n",
        "    if(param_cfg[\"OPTIMSCHEDULE\"]!=\"NONE\"):\n",
        "      scheduler.step()\n",
        "\n",
        "    if(i%eval_every==0):\n",
        "      valloss = 0.0\n",
        "      model.eval()\n",
        "      val_correct = 0\n",
        "      val_incorrect = 0\n",
        "      with torch.no_grad():\n",
        "        for data in valloader:\n",
        "          v_loss = 0.0\n",
        "          pred,protein_lengths = model(data,data.batch)\n",
        "          label = tmu.label_to_tensor(data.label,type2key)\n",
        "          label = torch.split(label,protein_lengths)\n",
        "          batch_sz = len(label)\n",
        "          for pred_, label_ in zip(pred,label):\n",
        "            v_loss += criterion(pred_,label_) #no need to worry about batching as graph is disjoint by design -> in principle no batching\n",
        "          v_loss /= batch_sz #normalize loss so it is corresponding to length of the batch\n",
        "          valloss += v_loss\n",
        "      if(param_cfg[\"BATCH_SZ\"]==1):\n",
        "         valloss /= len(valSet)\n",
        "      else:\n",
        "         valloss /= len(valloader) #also normalize using the number of batches\n",
        "\n",
        "      epoch_val_loss.append(valloss.item())\n",
        "\n",
        "\n",
        "      val_acc,val_acc_overlap,_,_,_,_,valMetrics = tmu.dataset_accuracy(model,valloader,type2key,metrics=True) #get accuracy\n",
        "      print(f\"Val accuracy/overlap/loss {val_acc}/{val_acc_overlap}/{valloss}\")\n",
        "      train_acc,train_acc_overlap = tmu.dataset_accuracy(model,trainloader,type2key)\n",
        "      #save results for report\n",
        "      epoch_train_acc_overlap.append(train_acc_overlap)\n",
        "      epoch_train_acc.append(train_acc)\n",
        "      epoch_val_acc_overlap.append(val_acc_overlap)\n",
        "      epoch_val_acc.append(val_acc)\n",
        "\n",
        "      print(f\"Train accuracy/overlap {train_acc}/{train_acc_overlap}\")\n",
        "      if(param_cfg[\"TRACKING\"]==True):\n",
        "        #get sample logs from VAL #after debugging this should maybe be wrapped in a function\n",
        "        pred,protein_lengths = model.predict(data,data.batch) #gives a list\n",
        "        label = tmu.label_to_tensor(data.label,type2key)\n",
        "        label = torch.split(label,protein_lengths)\n",
        "        log_nsamples = len(pred) #always get for a single batch\n",
        "        epoch_ = [i for x in range(log_nsamples)] #repeat so it matches\n",
        "        ids_, labels_, preds_, accuracy_, overlap_match_,label_type_,pred_type_= tmu.log_batch_elementwise_accuracies(pred,label,data.id)\n",
        "        if(param_cfg[\"DEBUG\"]):\n",
        "          print(f\"val: elementwise: {accuracy_}/{overlap_match_}\")\n",
        "        log_prediction_table(epoch_,ids_,labels_,preds_,accuracy_,overlap_match_,label_type_,pred_type_,type=\"val\")\n",
        "\n",
        "        #get sample logs from train\n",
        "        data = train_data_log #use same sample to not mess-up logging (else wandb believes another epoch has passed)\n",
        "        #print(\"OUTPUT FROM NEXT TRAINLOADER \",data)\n",
        "        pred, protein_lengths = model.predict(data,data.batch)\n",
        "        label = tmu.label_to_tensor(data.label,type2key)\n",
        "        label = torch.split(label,protein_lengths)\n",
        "\n",
        "        #if(param_cfg[\"BATCH_SZ\"]>1):\n",
        "        #   pred_unbatched = list(unbatch(pred,data.batch))\n",
        "        #else:\n",
        "        #   pred_unbatched = pred\n",
        "\n",
        "\n",
        "        ids_, labels_, preds_, accuracy_, overlap_match_,label_type_,pred_type_ = tmu.log_batch_elementwise_accuracies(pred,label,data.id)\n",
        "        if(param_cfg[\"DEBUG\"]):\n",
        "           print(f\"train: elementwise: {accuracy_}/{overlap_match_}\")\n",
        "        log_prediction_table(epoch_,ids_,labels_,preds_,accuracy_,overlap_match_,label_type_,pred_type_,type=\"train\")\n",
        "\n",
        "        val_metrics = {\"val/eval_epoch\": i,\n",
        "                    \"val/loss\":valloss,\n",
        "                     \"val/acc\":val_acc,\n",
        "                     \"val/acc_overlap\":val_acc_overlap,\n",
        "                     \"val/tm acc\":valMetrics[\"tm acc\"],\n",
        "                     \"val/tmsp acc\":valMetrics[\"sptm acc\"],\n",
        "                     \"val/sp acc\":valMetrics[\"sp acc\"],\n",
        "                     \"val/glob acc\":valMetrics[\"glob acc\"],\n",
        "                     \"val/beta acc\":valMetrics[\"beta acc\"],\n",
        "                     \"val/type acc\": valMetrics[\"type acc\"],\n",
        "                     \"val/tm type acc\":valMetrics[\"tm type acc\"],\n",
        "                     \"val/tmsp type acc\":valMetrics[\"tmsp type acc\"],\n",
        "                     \"val/sp type acc\":valMetrics[\"sp type acc\"],\n",
        "                     \"val/glob type acc\":valMetrics[\"glob type acc\"],\n",
        "                     \"val/beta type acc\":valMetrics[\"beta type acc\"]\n",
        "                     }\n",
        "\n",
        "\n",
        "        train_metrics = {\"train/eval_epoch\": i,\n",
        "                         \"train/acc\":train_acc,\n",
        "                     \"train/acc_overlap\":train_acc_overlap}\n",
        "\n",
        "        wandb.log(val_metrics)\n",
        "        wandb.log(train_metrics)\n",
        "\n",
        "\n",
        "      print(f\"Accuracies in epoch {i}: Train acc {train_acc}\\t Train overlap acc {train_acc_overlap}\")\n",
        "      model.train()\n",
        "\n",
        "      if(param_cfg[\"EARLYSTOP\"]):\n",
        "         early_stopping(val_acc_overlap,val_acc,model)\n",
        "         if early_stopping.early_stop:\n",
        "            print(f\"EPOCH {i}: Early stopping triggered. Model from epoch {i-30} saved to scratch...\")\n",
        "            del model\n",
        "            break\n",
        "\n",
        "\n",
        "if(param_cfg[\"Featuriser\"]==\"SIMPLE\"):\n",
        "  featuriser = ProteinFeaturiser( #note: input is a protein Batch\n",
        "          representation=\"CA\",\n",
        "          scalar_node_features=[\"amino_acid_one_hot\"],\n",
        "          vector_node_features=[],\n",
        "          edge_types=[\"knn_16\"],\n",
        "          scalar_edge_features=[\"edge_distance\"],\n",
        "          vector_edge_features=[],\n",
        "        )\n",
        "\n",
        "if(param_cfg[\"Featuriser\"]==\"INTERMEDIATE\"):\n",
        "  featuriser = ProteinFeaturiser( #note: input is a protein Batch\n",
        "          representation=\"CA\",\n",
        "          scalar_node_features=[\"amino_acid_one_hot\",\"sequence_positional_encoding\",\"alpha\",\"kappa\",\"dihedrals\"],\n",
        "          vector_node_features=[],\n",
        "          edge_types=[\"knn_16\"],\n",
        "          scalar_edge_features=[\"edge_distance\"],\n",
        "          vector_edge_features=[],\n",
        "        )\n",
        "\n",
        "if(param_cfg[\"Featuriser\"]==\"COMPLEX\"):#in general, according to paper, this should work less well than INTERMEDIATE\n",
        "  featuriser = ProteinFeaturiser( #note: input is a protein Batch\n",
        "          representation=\"CA\",\n",
        "          scalar_node_features=[\"amino_acid_one_hot\",\"sequence_positional_encoding\",\"alpha\",\"kappa\",\"dihedrals\",\"sidechain_torsions\"],\n",
        "          vector_node_features=[],\n",
        "          edge_types=[\"knn_16\"],\n",
        "          scalar_edge_features=[\"edge_distance\"],\n",
        "          vector_edge_features=[],\n",
        "        )\n",
        "\n",
        "\n",
        "if(param_cfg[\"EARLYSTOP\"]):\n",
        "   if(early_stopping.early_stop):  #if training was stopped due to early stopping we need to load best checkpoint\n",
        "      device = torch.device(\"cuda\")\n",
        "      model = GraphEncDec(featuriser=featuriser, n_classes=6,hidden_dim_GCN=param_cfg[\"GCSize\"],decoderType=param_cfg[\"DecoderType\"],LSTM_hidden_dim=param_cfg[\"LSTMSize\"],dropout=param_cfg[\"Dropout\"],LSTMnormalization = param_cfg[\"LSTMNORM\"],lstm_layers=param_cfg[\"LSTMLAYERS\"])\n",
        "      model.load_state_dict(torch.load(\"checkpoint.pt\"))\n",
        "      model = model.to(device)\n",
        "      #model.to(device)\n",
        "      model.eval()\n",
        "      print(\"Loaded best saved model\")\n",
        "\n",
        "else:\n",
        "   torch.save(model.state_dict(), \"checkpoint.pt\")\n",
        "   print(\"Early stopping never encountered/deactivated. Saved current state to scratch.\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Test-set evaluation"
      ],
      "metadata": {
        "id": "6-3Oo42xKfkE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "###-----------------------------------------------TEST MODEL\n",
        "model.eval()\n",
        "test_loss = 0.0\n",
        "test_loss_results = {}\n",
        "test_predictions = {}\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "with torch.no_grad():\n",
        "  sm = torch.nn.Softmax(dim=1)\n",
        "  if(param_cfg[\"TRACKING\"]==True):\n",
        "    test_sample_idx = [int(np.random.random()*param_cfg[\"BATCH_SZ\"])] #choose which batches to log to table\n",
        "  for i, data in enumerate(testloader): #always evaluated as single batch - maybe should be fixed\n",
        "    loss = 0.0\n",
        "    preds,protein_lengths = model(data,data.batch)\n",
        "    label = tmu.label_to_tensor(data.label,type2key) #numeric conversion\n",
        "    label = torch.split(label,protein_lengths) #list of numeric labels\n",
        "    pred_classes = []\n",
        "    for pred_, label_,id_,labelStr_ in zip(preds,label,data.id,data.label):\n",
        "       loss_ = criterion(pred_,label_) #no need to worry about batching as graph is disjoint by design -> in principle no batching\n",
        "       loss += loss_\n",
        "       pred_sm = sm(pred_)\n",
        "       pred_test_ = torch.argmax(pred_sm,dim=1)\n",
        "       pred_classes.append(pred_test_)\n",
        "\n",
        "       predStr = tmu.tensor_to_label(pred_test_,type2key)\n",
        "\n",
        "       proteinTypePred,pred_protein_label = tmu.type_from_labels(pred_test_)\n",
        "       proteinTypeLabel,true_protein_label = tmu.type_from_labels(label_)\n",
        "       preds_top = tmu.label_list_to_topology(pred_test_)\n",
        "       label_top = tmu.label_list_to_topology(label_)\n",
        "       match_tmp = tmu.is_topologies_equal(label_top,preds_top)\n",
        "       test_loss_results[id_.replace(\"_ABCD\",\"\")] = loss_.item()\n",
        "       test_predictions[id_.replace(\"_ABCD\",\"\")] = {\"prediction\":pred_test_.tolist(),\"label\":label_.tolist(),\"Type prediction\":pred_protein_label,\"Type label\":true_protein_label,\"Match\":match_tmp}\n",
        "    loss/=len(preds) #normalize for batch\n",
        "    test_loss += loss.item()\n",
        "    if(param_cfg[\"TRACKING\"]==True):\n",
        "        if i in test_sample_idx: #if this batch is to be saved to table\n",
        "            epoch_ = [param_cfg[\"N_EPOCHS\"] for x in range(len(pred_classes))]#repeat so it matches\n",
        "            ids_, labels_, preds_, accuracy_, overlap_match_,label_type_,pred_type_ = tmu.log_batch_elementwise_accuracies(pred_classes,label,data.id)\n",
        "            log_prediction_table(epoch_,ids_,labels_,preds_,accuracy_,overlap_match_,label_type_,pred_type_,type=\"test\")\n",
        "\n",
        "\n",
        "if(param_cfg[\"BATCH_SZ\"]==1):\n",
        "   mean_test_loss = test_loss/len(testSet)\n",
        "else:\n",
        "   mean_test_loss = test_loss/len(testloader) #batches\n",
        "\n",
        "#get accuracies across sets at end of experiment\n",
        "print(\"Evaluating train\")\n",
        "train_acc,train_acc_overlap,train_confmat_pr,train_confmat_type,train_AUROC,MCROC_train,trainMetrics = tmu.dataset_accuracy(model,trainloader,type2key,metrics=True)\n",
        "print(\"Evaluating val\")\n",
        "val_acc,val_acc_overlap,val_confmat_pr,val_confmat_type,val_AUROC,MCROC_val,valMetrics = tmu.dataset_accuracy(model,valloader,type2key,metrics=True)\n",
        "print(\"Evaluating test\")\n",
        "test_acc,test_acc_overlap,test_confmat_pr,test_confmat_type,test_AUROC,MCROC_test, testMetrics = tmu.dataset_accuracy(model,testloader,type2key,metrics=True)\n",
        "if(param_cfg[\"TRACKING\"]==True):\n",
        "   summary_titles = [\"f_test_Oacc\",\"f_val_Oacc\",\"f_train_Oacc\",\"f_test_acc\",\"f_val_acc\",\"f_train_acc\"]\n",
        "   #[\"f_train_acc\",\"f_train_Oacc\",\"f_val_acc\",\"f_val_Oacc\",\"f_test_acc\",\"f_test_Oacc\"]\n",
        "   #endresults = [train_acc,train_acc_overlap,val_acc,val_acc_overlap,test_acc,test_acc_overlap]\n",
        "   endresults = [test_acc_overlap,val_acc_overlap,train_acc_overlap,test_acc,val_acc,train_acc]\n",
        "   for i, name in enumerate(summary_titles):\n",
        "      wandb.summary[name] = endresults[i]\n",
        "   #for k, v in testMetrics.items(): <- old line\n",
        "   for k, v in valMetrics.items():\n",
        "      wandb.summary[k] = v\n",
        "\n",
        "\n",
        "\n",
        "#PLOT MULTICLASS ROC CURVES TO WANDB\n",
        "if(param_cfg[\"TRACKING\"]):\n",
        "   figtest_, axtest_ = MCROC_test.plot(score=True)\n",
        "   wandb.log({\"test/ROC\":wandb.Image(figtest_)})\n",
        "   del figtest_, axtest_\n",
        "\n",
        "   figtrain_, axtrain_ = MCROC_train.plot(score=True)\n",
        "   wandb.log({\"train/ROC\":wandb.Image(figtrain_)})\n",
        "   del figtrain_, axtrain_\n",
        "   figval_, axval_ = MCROC_val.plot(score=True)\n",
        "   wandb.log({\"val/ROC\":wandb.Image(figval_)})\n",
        "   del figval_, axval_\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "print(\"------------------- PR CONFUSION MATRICES TEST-SET----------------------\")\n",
        "print(test_confmat_pr)\n",
        "print(\"------------------- TYPE CONFUSION MATRICES TEST-SET----------------------\")\n",
        "print(test_confmat_type)\n",
        "print(\"------------------- AUROC TEST-SET----------------------\")\n",
        "print(test_AUROC)\n",
        "print(\"------------------- PR CONFUSION MATRICES VAL-SET----------------------\")\n",
        "print(val_confmat_pr)\n",
        "print(\"------------------- TYPE CONFUSION MATRICES VAL-SET----------------------\")\n",
        "print(val_confmat_type)\n",
        "print(\"------------------- AUROC VAL-SET----------------------\")\n",
        "print(val_AUROC)\n",
        "print(\"------------------- PR CONFUSION MATRICES TRAIN-SET----------------------\")\n",
        "print(train_confmat_pr)\n",
        "print(\"------------------- TYPE CONFUSION MATRICES TRAIN-SET----------------------\")\n",
        "print(train_confmat_type)\n",
        "print(\"------------------- AUROC TRAIN-SET----------------------\")\n",
        "print(train_AUROC)\n",
        "\n",
        "\n",
        "print(\"---------------------- ACCURACY METRICS ---------------------\")\n",
        "print(\"Train acc: \",train_acc)\n",
        "print(\"Train acc overlap: \",train_acc_overlap)\n",
        "print(\"Val acc: \",val_acc)\n",
        "print(\"Val acc overlap: \",val_acc_overlap)\n",
        "print(\"Test acc: \",test_acc)\n",
        "print(\"Test acc overlap: \",test_acc_overlap)\n",
        "\n",
        "\n",
        "if(param_cfg[\"TRACKING\"]==True):\n",
        "  sess_id = run.name\n",
        "  root_dir = \"\"\n",
        "  if(param_cfg[\"SAVERESULTS\"]):\n",
        "    tmu.saveResults(sess_id,root_dir,epoch_loss,epoch_train_acc,epoch_train_acc_overlap,epoch_val_loss,epoch_val_acc,epoch_val_acc_overlap,test_loss_results,test_predictions,\n",
        "                    train_acc,val_acc,test_acc,train_acc_overlap,val_acc_overlap,test_acc_overlap,\n",
        "                    param_cfg)\n",
        "    tmu.saveProteinMetrics(sess_id,root_dir,test_confmat_pr,test_confmat_type,testMetrics,SetType=\"test\")\n",
        "  wandb.finish()\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "r5d1zjFYKlUh"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}